
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=default'></script>

<link rel="stylesheet" type="text/css" href="style.css" />

<div class="topnav" id="myTopnav">
  <a href="https://www.nvidia.com/"><img width="100%" src="assets/nvidia.svg"></a>
  <a href="https://nv-tlabs.github.io/" ><strong>Toronto AI Lab</strong></a>
</div>

<!-- End : Google Analytics Code -->
<script type="text/javascript" src="../js/hidebib.js"></script>
<link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>
<head>
    <title>Frame Averaging for Equivariant Shape Space Learning</title>
    <meta property="og:description" content="Frame Averaging for Equivariant Shape Space Learning"/>
    <link href="https://fonts.googleapis.com/css2?family=Material+Icons" rel="stylesheet">

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-6HHDEXF452"></script>
<script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'G-6HHDEXF452');
</script>

</head>


 <body>
<div class="container">
    <div class="paper-title">
      <h1>Frame Averaging for Equivariant Shape Space Learning</h1>
    </div>

    
    <div id="authors">
        <div class="author-row">
            <div class="col-5 text-center"><a href="https://matanatz.github.io/">Matan Atzmon</a><sup>1,2</sup></div>
            <div class="col-5 text-center"><a href="https://luminohope.org/">Koki Nagano</a><sup>1</sup></div>
            <div class="col-5 text-center"><a href="https://www.cs.toronto.edu/~fidler/">Sanja Fidler</a><sup>1,3,4</sup></div>
            <div class="col-5 text-center"><a href="https://www.samehkhamis.com/">Sameh Khamis</a><sup>1</sup></div>
            <div class="col-5 text-center"><a href="https://www.wisdom.weizmann.ac.il/~ylipman/">Yaron Lipman</a><sup>2</sup></div>
        </div>

        <div class="affil-row">
            <div class="col-5 text-center"><sup>1</sup>NVIDIA</a></div>
            <div class="col-3 text-center"><sup>2</sup>Weizmann Institute of Science</a></div>
            <div class="col-5 text-center"><sup>3</sup>University of Toronto</div>
            <div class="col-5 text-center"><sup>4</sup>Vector Institute</div>
        </div>
        <!-- <div class="affil-row">
            <div class="venue text-center"><b>...</b></div>
        </div> -->

        <div style="clear: both">
            <div class="paper-btn-parent">
            <a class="supp-btn" href="https://arxiv.org/abs/2112.01741">
                <span class="material-icons"> description </span>
                 Paper
            </a>
            <a class="supp-btn" href="assets/bib.txt">
                <span class="material-icons"> description </span>
                  BibTeX
            </a>
        </div></div>
    </div>

    <section id="abstract"/>
        <h2>Abstract</h2>
        <hr/>

        <p>The task of shape space learning involves mapping a train set of shapes to and from a latent representation space
            with good generalization properties. Often, real-world collections of shapes have symmetries, which can be defined
            as transformations that do not change the essence of the shape. A natural way to incorporate symmetries in shape
            space learning is to ask that the mapping to the shape space (encoder) and mapping from the shape space (decoder) are
            equivariant to the relevant symmetries. In this paper, we present a framework for incorporating equivariance in
            encoders and decoders by introducing two contributions: (i) adapting the recent Frame Averaging (FA) framework for
            building generic, efficient, and maximally expressive Equivariant autoencoders; and (ii) constructing autoencoders
            equivariant to piecewise Euclidean motions applied to different parts of the shape. To the best of our knowledge,
            this is the first fully piecewise Euclidean equivariant autoencoder construction. Training our framework is simple:
            it uses standard reconstruction losses, and does not require the introduction of new losses. Our architectures are built
            of standard (backbone) architectures with the appropriate frame averaging to make them equivariant. Testing our
            framework on both rigid shapes dataset using implicit neural representations, and articulated shape datasets using
            mesh-based neural networks show state of the art generalization to unseen test shapes, improving relevant baselines
            by a large margin. In particular, our method demonstrates significant improvement in generalizing to unseen
            articulated poses.
            </p>
    </section>

    <section id="method">
        <h2>Method</h2>
        <hr/>
        <figure style="width: 100%;">
            <a href="assets/method_1.png">
                <img width="100%" src="assets/method_1.png">
            </a>
            <p class="caption" style="margin-bottom: 1.5rem">
                Built on the recent Frame Averaging approach, we present a unified framework for the
                construction of <strong>equivariant autoencoders</strong> with different 3D representations.
            </p>
        </figure>
        <hr/>

        <figure style="width: 100%;">
            <a href="assets/method_2.png">
                <img width="100%" src="assets/method_2.png">
            </a>
            <p class="caption" style="margin-bottom: 0.8rem">
                Novel construction of Piecewise Euclidean equivariant autoencoders. The suggested
                architecture consist of the same \(\phi\) backbone, where it is used for the equivariant
                encoding of each part. Similarly, the same \(\psi\) backbone is used for the equivariant
                decoding of each part’s latent code. Lastly, the final prediction is a weighted sum of each
                part’s equivariant output mesh.
            </p>
        </figure>
        </section>

        <section id="results"/>
        <h2>Results</h2>
        <hr/>

        <figure style="width: 100%;">    
            <a href="assets/commonobject3d.png">
                <img width="100%" src="assets/commonobject3d.png">
            </a>
            <p class="caption" style="margin-bottom: 1.5rem">
            <strong>CommonObject3D dataset</strong>: Global Euclidean equivariant point cloud \(\rightarrow\) implicit.
            </p>
        </figure>
        
        <hr/>

        <figure style="width: 100%;">    
            <a href="assets/dfaust.png">
                <img width="100%" src="assets/dfaust.png">
            </a>
            <p class="caption" style="margin-bottom: 0.8rem">
            <strong>DFaust dataset</strong>: Piecewise Euclidean equivariant mesh \(\rightarrow\) mesh. Testing on poses that are unseen during training.
            </p>
        </figure>

        <hr/>
        <figure style="width: 100%;">    
            <a href="assets/interpolation.png">
                <img width="100%" src="assets/interpolation.png">
            </a>
            <p class="caption" style="margin-bottom: 0.8rem">
                Interpolation in equivariant latent space between two test examples.
            </p>
        </figure>

        <hr/>
        <figure style="width: 100%;">  
            <row>
            <div class="col-2">
                <video width="100%" controls muted loop autoplay>
                    <source src="assets/ours.mp4" type="video/mp4">
                </video>
                <p class="text-center">Ours</p>
            </div>
            <div class="col-2">
                <video width="100%" controls muted loop autoplay>
                    <source src="assets/snarf.mp4" type="video/mp4">
                </video>
                <p class="text-center">SNARF</p>
            </div>
            </row>

            <p class="caption" style="margin-bottom: 0.8rem">
                Comparison with an implicit pose-conditioned method (SNARF).
            </p>
        </figure>

    </section>

    <section id="bibtex">
        <h2>Citation</h2>
        <hr/>
        <pre><code>
        @misc{atzmon2021equivariant,
            title = {Frame Averaging for Equivariant Shape Space Learning}, 
            author = {Matan Atzmon and Koki Nagano and Sanja Fidler and Sameh Khamis and Yaron Lipman},
            eprint = {2112.01741},
            archivePrefix={arXiv},
            primaryClass={cs.CV},
            year = {2021}
        }
</code></pre>
    </section>

    <section id="paper">
        <h2>Paper</h2>
        <hr/>
        <figure style="width: 100%;">
            <a href="https://arxiv.org/abs/2112.01741">
                <img width="100%" src="assets/preview.png">
            </a>
        </figure>
        <hr/>
    </section>

</div>
</body>
</html>
